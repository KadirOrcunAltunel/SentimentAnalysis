Sentiment analysis has been subject to various research due to its popularity in capturing emotional tone in a text. A recent study by Das et al. (2023) pointed out that logistic regression combined with TF-IDF (Term Frequency-Inverse Document Frequency), or word embeddings could be a powerful model for sentiment analysis. However, because logistic regression uses manually engineered features like BoW (Bag of Words) and TF-IDF, it may not capture contextual meaning, word order, or relationships between words. Therefore, it should be considered a baseline model considering its simplicity and interpretability. 

With its feature engineering capabilities and effective handling of class imbalance, XGBoost has emerged as a strong alternative for sentiment analysis. Wang et al. (2019) showed that XGBoost could provide high precision in sentiment classification. Nevertheless, the fact that XGBoost struggles with capturing sequential data limits its use on raw textual data, prompting a search for alternatives. 

LSTM's capability to capture long-term dependencies in sentiment analysis tasks has garnered significant interest in academia. Cao et al. (2020) showed that LSTM networks are effective in sentiment analysis especially combined with word embeddings such as GloVe. However, vanishing gradient problems and the requirement of powerful computational resources can pose challenges for LSTM in sentiment analysis. However, as Hameed et al. (2020) has demonstrated, thanks to the introduction of BiLSTM (Bidirectional LSTM) models, the performance of LSTM networks has improved drastically in sentiment analysis. 

One of the biggest innovations that has revolutionized sentiment analysis is the introduction of transformers in Large Language Models. Recent study by Souza et al. (2022) showed that BERT is an effective architecture in sentiment analysis achieving sophisticated results across various domains. BERT has also shown a remarkable improvement in handling ambiguous language which traditional deep learning models struggle with. Despite the success of BERT in sentiment analysis, it remains less preferred due to being computationally expensive. 

RoBERta, which is an optimized version of BERT provides even better performance in sentiment analysis by training on larger datasets and longer sequences. Tarunesh et al. (2021) demonstrated RoBERTa’s ability to outperform BERT in dealing with complex language. This is particularly thanks to the elimination of Next Sentence Prediction (NSP) task used in BERT, leading to a faster fine tuning.

However, current methods for performing sentiment analysis face several key challenges:

• **Traditional methods are limited**: Relying on manual feature engineering techniques like TF-IDF can be time consuming and it can fail to capture the subtle meaning and context important for understanding sentiment.

• **Sequential models lack integration**: Models like LSTM, while effective, are often treated separately from pre-processing steps. This leads to poor workflows and potential loss of valuable information.

• **Advanced models overlook imbalance**: Powerful transformer- based models like BERT and RoBERTa frequently struggle with the uneven distribution of sentiment classes. For instance, Bashiri et al. (2024) discusses the limitations of transformers in dealing with class imbalance and highlights the need for specialized techniques.

In this project, I proposed a framework to address these limitations by using a combination of traditional and modern approaches. My methodology integrated class-weighted logistic regression and XGBoost with TF-IDF for feature extraction, BiLSTM with GloVe embeddings for capturing sequential dependencies, and transformer architectures like BERT and RoBERTa for their bidirectional contextual understanding. Additionally, I introduced focal loss and other imbalance-handling strategies to improve F1-score on minority classes, which is crucial in scenarios where underrepresented sentiments can hold significant importance.
